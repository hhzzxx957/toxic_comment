# toxic_comment_analysis_by_LSTM
This project used LSTM to analysis comments' toxicity from Wikipedia, which originaly bring up by this kaggle competition
(https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge#description)

## notebook of LSTM implementation and exploratory data analysis
https://github.com/hhzzxx957/toxic_comment/blob/master/codes/improved-lstm-baseline-glove-dropout.ipynb

## Examples about Esemble modeling

Here are the example codes used in the kaggle competition, input files are not included.
### Out-of-folder(OOF) stacking
https://github.com/hhzzxx957/toxic_comment/blob/master/codes/oof_stack_regime.ipynb
### Geomean and wighted average
https://github.com/hhzzxx957/toxic_comment/blob/master/codes/geomean.ipynb

## A simple website to test comment's toxicity
http://hzx957.pythonanywhere.com/
